***Use Keras for the Tensorflow frontend**

Our best architecture consists of eight convolutional hidden layers, one locally connected hidden layer, and two densely connected hidden layers. All connections are feedforward and go from one layer to the next (no skip connections).

The first hidden layer contains maxout units (Goodfellow et al., 2013) (with three filters per unit) while the others contain rectifier units (Jarrett et al., 2009; Glorot et al., 2011).

The number of units at each spatial location in each layer is [48, 64, 128, 160] for the first four layers and 192 for all other locally connected layers. The fully connected layers contain 3,072 units each.

Each convolutional layer includes max pooling and subtractive normalization.  All convolution kernels were of size 5 × 5. The max pooling window size is 2 × 2. The stride alternates between 2 and 1 at each layer, so that half of the layers don’t reduce the spatial size of the representation.

All convolutions use zero padding on the input to preserve representation size.

The subtractive normalization operates on 3x3 windows and preserves representation size.

We trained with dropout applied to all hidden layers but not the input.

* Note that "zero-pad" is referred to as "same" padding by TensorFlow
* Note further that Keras' `MaxoutDense` Layer may be the maxout layer referred to below
* See Keras' [pooling layers](https://keras.io/layers/pooling/) for pooling
* Is "subtractive normalization" Keras' `BatchNormalization`?
* Keras appears to have [locally-connected](https://keras.io/layers/local/) layers
* Keras has ReLU and dropout

* Flatten between last conv and first locally-connected?

* Goodfellow conditional probabilistic output layer in Keras?
* Goodfellow et al model only outputs 5 digit MAX, therefore there will need to be 6 output nodes: 1 for length, and 5 conditionally-activated nodes for digits, which share weights and biases.
  * Note, though, that the Length can take values from 0, 1, 2, 3, 4, 5, and more than 5, which is a total of 7 classes.

* Maximize the **logarithm** of the probability of (the given sequence of digits, S) given (the input image, X)
* Use "a generic method, like stochastic gradient descent"
* Softmax outputs probabilities?

(0) input (54 x 54 x 3 image)
(1) same-pad 5 × 5 conv  [48] -> 3-filter maxout (?) -> 2 × 2 max pooling -> 3 × 3 subtractive normalization -> dropout
(2) same-pad 5 × 5 conv  [64] -> 2 × 2 max pooling (stride 2) -> 3 × 3 subtractive normalization -> ReLU -> dropout
(3) same-pad 5 × 5 conv [128] -> 2 × 2 max pooling (stride 1) -> 3 × 3 subtractive normalization -> ReLU -> dropout
(4) same-pad 5 × 5 conv [160] -> 2 × 2 max pooling (stride 2) -> 3 × 3 subtractive normalization -> ReLU -> dropout
(5) same-pad 5 × 5 conv [192] -> 2 × 2 max pooling (stride 1) -> 3 × 3 subtractive normalization -> ReLU -> dropout
(6) same-pad 5 × 5 conv [192] -> 2 × 2 max pooling (stride 2) -> 3 × 3 subtractive normalization -> ReLU -> dropout
(7) same-pad 5 × 5 conv [192] -> 2 × 2 max pooling (stride 1) -> 3 × 3 subtractive normalization -> ReLU -> dropout
(8) same-pad 5 × 5 conv [192] -> 2 × 2 max pooling (stride 2) -> 3 × 3 subtractive normalization -> ReLU -> dropout
(9) locally-connected [192] -> ReLU -> dropout
(10) fully-connected [3072] -> ReLU -> dropout
(11) fully-connected [3072] -> ReLU -> dropout
(12) output
